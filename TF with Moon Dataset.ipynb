{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_moons(10000, noise=0.15)\n",
    "\n",
    "# Can't believe I'm doing it this way, but join doesn't work\n",
    "# on numpy strings and I'm on a plane unable to lookup the\n",
    "# right way to join a column to a matrix and output as CSV.\n",
    "for x_i,y_i in zip(X,y):\n",
    "    output = ''\n",
    "    output += str(y_i)\n",
    "    for j in range(0,len(x_i)):\n",
    "        output += ','\n",
    "        output += str(x_i[j])\n",
    "    # print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NObs =X.shape[0]\n",
    "\n",
    "traindata=X[0:NObs/2,:]\n",
    "validdata=X[NObs/2+1:NObs*3/4,:]\n",
    "testdata=X[NObs*3/4+1:,:]\n",
    "\n",
    "train_labels=pd.DataFrame(y[0:NObs/2])\n",
    "valid_labels=pd.DataFrame(y[NObs/2+1:NObs*3/4])\n",
    "\n",
    "test_labels=pd.DataFrame(y[NObs*3/4+1:])\n",
    "\n",
    "train_labels.columns = ['label']\n",
    "valid_labels.columns = ['label']\n",
    "test_labels.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare data for TensorFlow\n",
    "\n",
    "num_var = 2\n",
    "num_labels = 2 # 39 categories\n",
    "\n",
    "train_dataset = np.float32(np.array(traindata))\n",
    "N_obs_train=len(train_dataset)\n",
    "\n",
    "valid_dataset = np.float32(np.array(validdata))\n",
    "\n",
    "test_dataset = np.float32(np.array(testdata))\n",
    "\n",
    "train_labels= np.array(pd.get_dummies(train_labels['label'].astype('category').cat.rename_categories(np.arange(0,num_labels)) ))\n",
    "valid_labels= np.array(pd.get_dummies(valid_labels['label'].astype('category').cat.rename_categories(np.arange(0,num_labels)) ))\n",
    "test_labels= np.array(pd.get_dummies ( test_labels['label'].astype('category').cat.rename_categories(np.arange(0,num_labels)) ))\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape)\n",
    "\n",
    "test_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def baseline_logloss(hotmatrix): #input is a hot-encoded sparse matrix, one 1 only per row, rest is 0\n",
    "    nobs = len(hotmatrix)\n",
    "    temp = hotmatrix.sum(axis=0)/nobs\n",
    "    return(-sum(np.multiply(temp,np.log(temp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding regularization to the 1 hidden layer network\n",
    "graph1 = tf.Graph()\n",
    "batch_size = 128\n",
    "\n",
    "epoch_size  = N_obs_train / batch_size\n",
    "num_steps= 5 * epoch_size\n",
    "\n",
    "import datetime\n",
    "startTime = datetime.datetime.now() \n",
    "\n",
    "def define_and_run_batch(beta):\n",
    "    \n",
    "    num_RELU = 10\n",
    "    \n",
    "    with graph1.as_default():\n",
    "      \n",
    "      keep_prob = tf.placeholder(tf.float32)\n",
    "      \n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, num_var)) # remplace batch_size par None\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "      # in Kaggle competitions, we don't know the true labels of the test set\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "      # Variables.\n",
    "      weights_RELU = tf.Variable(\n",
    "        tf.truncated_normal([num_var, num_RELU],stddev=0.1))\n",
    "        \n",
    "      # print(tf.shape(weights_RELU) )   \n",
    "      #biases_RELU = tf.Variable(tf.zeros([num_RELU]))\n",
    "      biases_RELU = tf.Variable(tf.random_normal([num_RELU],stddev=0.1))  \n",
    "        \n",
    "      weights_layer1 = tf.Variable(\n",
    "        tf.truncated_normal([num_RELU, num_labels],stddev=0.1))\n",
    "      #biases_layer1 = tf.Variable(tf.zeros([num_labels]))\n",
    "      biases_layer1 = tf.Variable(tf.random_normal([num_labels],stddev=0.1))\n",
    "  \n",
    "      # Training computation.\n",
    "      logits_RELU = tf.matmul(tf_train_dataset, weights_RELU) + biases_RELU\n",
    "      RELU_vec = tf.nn.relu(logits_RELU)\n",
    "      RELU_vec_drop = tf.nn.dropout(RELU_vec, keep_prob)\n",
    "    \n",
    "      logits_layer = tf.matmul(RELU_vec_drop, weights_layer1) + biases_layer1                  \n",
    "      # loss = tf.reduce_mean(\n",
    "      #        tf.nn.softmax_cross_entropy_with_logits(logits_layer, tf_train_labels))\n",
    "      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits_layer, tf_train_labels,name=\"cross_entropy\")\n",
    "      l2reg = tf.reduce_sum(tf.square(weights_RELU))+tf.reduce_sum(tf.square(weights_layer1))\n",
    "      \n",
    "      loss = tf.reduce_mean(cross_entropy+beta*l2reg)\n",
    "      logloss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "  # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(0.3).minimize(loss)\n",
    "  \n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits_layer)\n",
    "      \n",
    "      valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu((tf.matmul(tf_valid_dataset, weights_RELU) + biases_RELU)),weights_layer1)+biases_layer1)\n",
    "        \n",
    "      # in Kaggle competitions, we don't know the true labels of the test set     \n",
    "      test_prediction =tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu((tf.matmul(tf_test_dataset, weights_RELU) + biases_RELU)),weights_layer1)+biases_layer1)\n",
    "                         \n",
    "    with tf.Session(graph=graph1) as session:\n",
    " \n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch. \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        \n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n",
    "                    keep_prob:1.0}\n",
    "        \n",
    "        #\n",
    "        if (step==num_steps-1):\n",
    "            _, l, predictions_train,predictions_test,predictions_valid = session.run(\n",
    "              [optimizer, logloss,train_prediction,test_prediction,valid_prediction], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, predictions_train = session.run(\n",
    "              [optimizer, logloss,train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        if (step % 50 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions_train, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      # test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "      # print(\"Test accuracy: %.1f%%\" % test_acc)\n",
    "\n",
    "      \n",
    "        \n",
    "    x = datetime.datetime.now() - startTime\n",
    "    print(x)\n",
    "    return(predictions_test,predictions_valid,predictions_train)\n",
    "    \n",
    "    \n",
    "RES,RES_v,RES_tr=define_and_run_batch(0.001)\n",
    "N_obs_valid = validdata.shape[0]\n",
    "N_obs_test  =  testdata.shape[0]\n",
    "logloss_valid=-sum(sum(np.multiply(valid_labels,np.log(RES_v))))/N_obs_valid\n",
    "logloss_test =-sum(sum(np.multiply( test_labels,np.log(RES  ))))/N_obs_test\n",
    "print(\"Validation Logloss =%s\" % logloss_valid)\n",
    "print(\"Test       Logloss =%s\" % logloss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 layer network\n",
    "\n",
    "graph1 = tf.Graph()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "epoch_size  = N_obs_train / batch_size\n",
    "num_steps= 2 * epoch_size #6501\n",
    "\n",
    "import datetime\n",
    "startTime = datetime.datetime.now() \n",
    "\n",
    "def define_and_run_batch_4_layer():\n",
    "    \n",
    "    \n",
    "\n",
    "    import math as math\n",
    "\n",
    "    initial_learning_rate_value = 0.5\n",
    "    beta1 = 0.001\n",
    "    beta2 = 0.001\n",
    "    beta3 = 0.001\n",
    "    beta4 = 0.001\n",
    "    beta5 = 0.001\n",
    "\n",
    "    graph2 = tf.Graph()\n",
    "    with graph2.as_default():\n",
    "\n",
    "       # Input data. For the training data, we use a placeholder that will be fed\n",
    "       # at run time with a training minibatch.\n",
    "       tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_var))\n",
    "       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        \n",
    "       tf_valid_dataset = tf.constant(valid_dataset)\n",
    "       tf_test_dataset = tf.constant(test_dataset)    \n",
    "    \n",
    "       # learning rate decay\n",
    "       global_step = tf.Variable(0)\n",
    "       learning_rate = tf.train.exponential_decay(initial_learning_rate_value, global_step, 1, 0.9999)\n",
    "\n",
    "       # Hidden layer 1\n",
    "       h1_size = 32  \n",
    "       weights_h1 = tf.Variable(tf.truncated_normal([num_var, h1_size], stddev=math.sqrt(2.0/(num_var))))\n",
    "       biases_h1 = tf.Variable(tf.zeros([h1_size]))\n",
    "       h1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_h1) + biases_h1)\n",
    "\n",
    "       # Hidden layer 2\n",
    "       h2_size = 16\n",
    "       weights_h2 = tf.Variable(tf.truncated_normal([h1_size, h2_size], stddev=math.sqrt(2.0/h1_size)))\n",
    "       biases_h2 = tf.Variable(tf.zeros([h2_size]))\n",
    "       h2 = tf.nn.relu(tf.matmul(h1, weights_h2) + biases_h2)\n",
    "\n",
    "       # Hidden layer 3\n",
    "       h3_size = 8\n",
    "       weights_h3 = tf.Variable(tf.truncated_normal([h2_size, h3_size], stddev=math.sqrt(2.0/h2_size)))\n",
    "       biases_h3 = tf.Variable(tf.zeros([h3_size]))\n",
    "       h3 = tf.nn.relu(tf.matmul(h2, weights_h3) + biases_h3)\n",
    "\n",
    "       # Hidden layer 4\n",
    "       h4_size = 4\n",
    "       weights_h4 = tf.Variable(tf.truncated_normal([h3_size, h4_size], stddev=math.sqrt(2.0/h3_size)))\n",
    "       biases_h4 = tf.Variable(tf.zeros([h4_size]))\n",
    "       h4 = tf.nn.relu(tf.matmul(h3, weights_h4) + biases_h4)\n",
    "\n",
    "       # Output layer\n",
    "       weights_o = tf.Variable(tf.truncated_normal([h4_size, num_labels], stddev=math.sqrt(2.0/h4_size)))\n",
    "       biases_o = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "       # Training computation.\n",
    "       logits = tf.matmul(h4, weights_o) + biases_o\n",
    "       # loss = tf.reduce_mean(\n",
    "       #     tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + ((beta1 * tf.nn.l2_loss(weights_h1)) + (beta2 * tf.nn.l2_loss(weights_h2)) \n",
    "       #        + (beta3 * tf.nn.l2_loss(weights_h3)) + (beta4 * tf.nn.l2_loss(weights_h4)) + (beta5 * tf.nn.l2_loss(weights_o)))\n",
    "       \n",
    "       cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels,name=\"cross_entropy\")\n",
    "       logloss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "       loss = logloss + ((beta1 * tf.nn.l2_loss(weights_h1)) + (beta2 * tf.nn.l2_loss(weights_h2)) + (beta3 * tf.nn.l2_loss(weights_h3)) \n",
    "                         + (beta4 * tf.nn.l2_loss(weights_h4)) + (beta5 * tf.nn.l2_loss(weights_o)))\n",
    "       \n",
    "    \n",
    "       # Optimizer.\n",
    "       optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "       # Predictions for the training, validation, and test data.\n",
    "       train_h1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_h1) + biases_h1)\n",
    "       train_h2 = tf.nn.relu(tf.matmul(train_h1, weights_h2) + biases_h2)\n",
    "       train_h3 = tf.nn.relu(tf.matmul(train_h2, weights_h3) + biases_h3)\n",
    "       train_h4 = tf.nn.relu(tf.matmul(train_h3, weights_h4) + biases_h4)\n",
    "       train_logits = tf.matmul(train_h4, weights_o) + biases_o\n",
    "       train_prediction = tf.nn.softmax(train_logits)\n",
    "  \n",
    "       valid_h1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_h1) + biases_h1)\n",
    "       valid_h2 = tf.nn.relu(tf.matmul(valid_h1, weights_h2) + biases_h2)\n",
    "       valid_h3 = tf.nn.relu(tf.matmul(valid_h2, weights_h3) + biases_h3)\n",
    "       valid_h4 = tf.nn.relu(tf.matmul(valid_h3, weights_h4) + biases_h4)\n",
    "       valid_logits = tf.matmul(valid_h4, weights_o) + biases_o\n",
    "       valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "       test_h1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_h1) + biases_h1)\n",
    "       test_h2 = tf.nn.relu(tf.matmul(test_h1, weights_h2) + biases_h2)\n",
    "       test_h3 = tf.nn.relu(tf.matmul(test_h2, weights_h3) + biases_h3)\n",
    "       test_h4 = tf.nn.relu(tf.matmul(test_h3, weights_h4) + biases_h4)\n",
    "       test_logits = tf.matmul(test_h4, weights_o) + biases_o\n",
    "       test_prediction = tf.nn.softmax(test_logits)\n",
    "  \n",
    "        \n",
    "   \n",
    "\n",
    "# executing the graph     \n",
    "        \n",
    "        \n",
    "    with tf.Session(graph=graph2) as session:\n",
    " \n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch. \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        \n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #\n",
    "        if (step==num_steps-1):\n",
    "            _, l, predictions_train,predictions_test,predictions_valid = session.run(\n",
    "              [optimizer, logloss,train_prediction,test_prediction,valid_prediction], feed_dict=feed_dict)\n",
    "        else:\n",
    "            _, l, predictions_train = session.run(\n",
    "              [optimizer, logloss,train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions_train, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      # test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "      # print(\"Test accuracy: %.1f%%\" % test_acc)\n",
    "        \n",
    "    x = datetime.datetime.now() - startTime\n",
    "    print(x)\n",
    "    return(predictions_test,predictions_valid)\n",
    "    \n",
    "    \n",
    "    \n",
    "RES,RES_v=define_and_run_batch_4_layer()\n",
    "\n",
    "N_obs_valid = validdata.shape[0]\n",
    "N_obs_test  =  testdata.shape[0]\n",
    "logloss_valid=-sum(sum(np.multiply(valid_labels,np.log(RES_v))))/N_obs_valid\n",
    "logloss_test =-sum(sum(np.multiply( test_labels,np.log(RES  ))))/N_obs_test\n",
    "print(\"Validation Logloss =%s\" % logloss_valid)\n",
    "print(\"Test       Logloss =%s\" % logloss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(baseline_logloss(train_labels))\n",
    "print(baseline_logloss(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy(RES_tr,train_labels))\n",
    "print(accuracy(RES_v ,valid_labels))\n",
    "print(accuracy(RES   , test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a4ad78816764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNObs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RES' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(testdata)\n",
    "df.columns=['X','Y']\n",
    "df['pred']=np.argmax(RES,1)\n",
    "df['label']=y[NObs*3/4+1:]\n",
    "\n",
    "def confusion (pred,label):\n",
    "    if    (pred==1) and (label==1): return ('true_pos')\n",
    "    elif  (pred==1) and (label==0): return ('false_pos')\n",
    "    elif  (pred==0) and (label==0): return ('true_neg')\n",
    "    else                          : return ('false_neg')\n",
    "\n",
    "def myfun(row):\n",
    "    return(confusion(row['pred'],row['label']))\n",
    "df['class']=df.apply(myfun,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(df['X'],df['Y'],'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in grouped:\n",
    "    ax.plot(group.X, group.Y, marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2692, 1153],\n",
       "       [1737,  699],\n",
       "       [ 524,  522],\n",
       "       ..., \n",
       "       [4210, 1785],\n",
       "       [3698, 1160],\n",
       "       [1061, 2043]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances,indices = nbrs.kneighbors(testdata)\n",
    "indices\n",
    "#distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh=KNeighborsClassifier(n_neighbors=3,metric='euclidean')\n",
    "y=np.array(train_labels).reshape(5000,)\n",
    "neigh.fit(traindata,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98719487795118044"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict(testdata)\n",
    "neigh.score(testdata,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mydist(x,y):\n",
    "    z=x-y\n",
    "    return(z[0]**2+ 2*z[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([1,1])\n",
    "y=np.array([2,2])\n",
    "z=x-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neigh=KNeighborsClassifier(n_neighbors=3,metric=mydist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "           metric=<function mydist at 0x1098a6140>, metric_params=None,\n",
       "           n_jobs=1, n_neighbors=3, p=2, weights='uniform')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(traindata,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98799519807923164"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.predict(testdata)\n",
    "neigh.score(testdata,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=2, n_init=3,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters=2,n_init=3)\n",
    "km.fit(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.predict(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(traindata)\n",
    "df1.columns=['X','Y']\n",
    "df1['cluster']=km.predict(traindata)\n",
    "grouped1 = df1.groupby('cluster')\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in grouped1:\n",
    "    ax.plot(group.X, group.Y, marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.060217</td>\n",
       "      <td>0.209108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.439533</td>\n",
       "      <td>-0.329381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559930</td>\n",
       "      <td>0.847939</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549476</td>\n",
       "      <td>0.405953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036195</td>\n",
       "      <td>0.359942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.917799</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.266213</td>\n",
       "      <td>-0.110192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.301464</td>\n",
       "      <td>1.084138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.878780</td>\n",
       "      <td>0.177226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.035439</td>\n",
       "      <td>0.111614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.993979</td>\n",
       "      <td>-0.689498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.607956</td>\n",
       "      <td>0.954872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.195440</td>\n",
       "      <td>1.095297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.352856</td>\n",
       "      <td>-0.710316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.495402</td>\n",
       "      <td>-0.245154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.401941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.132731</td>\n",
       "      <td>1.104167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.964956</td>\n",
       "      <td>0.366122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.500038</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.182075</td>\n",
       "      <td>0.927184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.213295</td>\n",
       "      <td>0.856715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.061805</td>\n",
       "      <td>0.553391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.521456</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.310884</td>\n",
       "      <td>0.240976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.025935</td>\n",
       "      <td>-0.262536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.363754</td>\n",
       "      <td>1.030999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.304972</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.034327</td>\n",
       "      <td>0.093469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.881127</td>\n",
       "      <td>0.370273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.920104</td>\n",
       "      <td>0.134255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>0.427984</td>\n",
       "      <td>0.853803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>0.464986</td>\n",
       "      <td>0.796056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>-0.529960</td>\n",
       "      <td>0.529819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>-0.335219</td>\n",
       "      <td>0.860164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>1.016261</td>\n",
       "      <td>0.101313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>-0.771137</td>\n",
       "      <td>0.817071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>0.879484</td>\n",
       "      <td>0.082616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>-0.547878</td>\n",
       "      <td>0.933392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>-0.986933</td>\n",
       "      <td>0.135949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>1.147784</td>\n",
       "      <td>-0.507201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>0.913653</td>\n",
       "      <td>-0.616283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.431334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>1.260525</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>-0.293099</td>\n",
       "      <td>1.101079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>0.175167</td>\n",
       "      <td>0.057280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>0.112189</td>\n",
       "      <td>1.093890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>0.390089</td>\n",
       "      <td>0.785737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>0.610598</td>\n",
       "      <td>1.086911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>0.188713</td>\n",
       "      <td>1.082374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>0.860864</td>\n",
       "      <td>-0.553143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>1.196617</td>\n",
       "      <td>-0.609913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>0.652994</td>\n",
       "      <td>1.127841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>-0.840211</td>\n",
       "      <td>0.490550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>1.224510</td>\n",
       "      <td>0.242679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>-0.879107</td>\n",
       "      <td>0.343996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2.144139</td>\n",
       "      <td>0.530936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.446206</td>\n",
       "      <td>0.798540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.019491</td>\n",
       "      <td>0.957773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.537978</td>\n",
       "      <td>-0.197330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-0.739619</td>\n",
       "      <td>0.579923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X         Y  cluster\n",
       "0    -1.060217  0.209108        1\n",
       "1     1.439533 -0.329381        0\n",
       "2     0.559930  0.847939        1\n",
       "3     0.549476  0.405953        1\n",
       "4    -0.036195  0.359942        1\n",
       "5     0.917799 -0.001895        0\n",
       "6     0.266213 -0.110192        1\n",
       "7    -0.301464  1.084138        1\n",
       "8    -0.878780  0.177226        1\n",
       "9     0.035439  0.111614        1\n",
       "10    0.993979 -0.689498        0\n",
       "11   -0.607956  0.954872        1\n",
       "12   -0.195440  1.095297        1\n",
       "13    1.352856 -0.710316        0\n",
       "14    1.495402 -0.245154        0\n",
       "15    0.066671  0.401941        1\n",
       "16    0.132731  1.104167        1\n",
       "17    1.964956  0.366122        0\n",
       "18    0.500038  0.862709        1\n",
       "19    0.182075  0.927184        1\n",
       "20   -0.213295  0.856715        1\n",
       "21    1.061805  0.553391        0\n",
       "22   -0.521456  0.983262        1\n",
       "23    1.310884  0.240976        0\n",
       "24    2.025935 -0.262536        0\n",
       "25   -0.363754  1.030999        1\n",
       "26   -0.304972  0.742500        1\n",
       "27    2.034327  0.093469        0\n",
       "28    1.881127  0.370273        0\n",
       "29   -0.920104  0.134255        1\n",
       "...        ...       ...      ...\n",
       "4970  0.427984  0.853803        1\n",
       "4971  0.464986  0.796056        1\n",
       "4972 -0.529960  0.529819        1\n",
       "4973 -0.335219  0.860164        1\n",
       "4974  1.016261  0.101313        0\n",
       "4975 -0.771137  0.817071        1\n",
       "4976  0.879484  0.082616        0\n",
       "4977 -0.547878  0.933392        1\n",
       "4978 -0.986933  0.135949        1\n",
       "4979  1.147784 -0.507201        0\n",
       "4980  0.913653 -0.616283        0\n",
       "4981  0.006328  0.431334        1\n",
       "4982  1.260525  0.035562        0\n",
       "4983 -0.293099  1.101079        1\n",
       "4984  0.175167  0.057280        1\n",
       "4985  0.112189  1.093890        1\n",
       "4986  0.390089  0.785737        1\n",
       "4987  0.610598  1.086911        1\n",
       "4988  0.188713  1.082374        1\n",
       "4989  0.860864 -0.553143        0\n",
       "4990  1.196617 -0.609913        0\n",
       "4991  0.652994  1.127841        1\n",
       "4992 -0.840211  0.490550        1\n",
       "4993  1.224510  0.242679        0\n",
       "4994 -0.879107  0.343996        1\n",
       "4995  2.144139  0.530936        0\n",
       "4996 -0.446206  0.798540        1\n",
       "4997  0.019491  0.957773        1\n",
       "4998  0.537978 -0.197330        0\n",
       "4999 -0.739619  0.579923        1\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2^4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
